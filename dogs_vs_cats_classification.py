# -*- coding: utf-8 -*-
"""Dogs-vs-Cats-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCwsTTjmXXiXiRdvC_TDMO4huN5sfMyo

### Collecting dataset from Kaggle using API key
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d salader/dogs-vs-cats

"""### Unzipping the Zip file of kaggle dataset"""

import zipfile
zip_ref = zipfile.ZipFile("/content/dogs-vs-cats.zip")
zip_ref.extractall("/content")
zip_ref.close()

import tensorflow as tf
from keras import Sequential
from tensorflow import keras
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization

"""##### Lets Use Generators from keras to process the dataset of images which is reliable as it makes batches of data and executed it eventually"""

## Generators

train_dataset = keras.utils.image_dataset_from_directory(
    directory = '/content/train',
    labels = 'inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256,256)
)

validation_dataset = keras.utils.image_dataset_from_directory(
    directory = '/content/test',
    labels = 'inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256,256)
)

"""##### Now the data is from 0-255 in numpy array, we need to normalize to 0-1"""

## Normalize

def process(image, labels):
  image = tf.cast(image/255., tf.float32)
  return image, labels
train_dataset = train_dataset.map(process)
validation_dataset = validation_dataset.map(process)

"""Now our dataset is ready

### Lets create CNN model
"""

model = Sequential()
model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu', input_shape= (256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid'))

model.add(Conv2D(64, kernel_size=(3,3), padding='valid', activation='relu', input_shape= (256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid'))

model.add(Conv2D(128, kernel_size=(3,3), padding='valid', activation='relu', input_shape= (256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size = (2,2), strides=2, padding='valid'))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(train_dataset, epochs=10, validation_data=validation_dataset)

"""### Previous Output:
Seems like the model is overfitting as the training accuracy is increasing but the validation accuracy is not increasing.
"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], color='red', label='train')
plt.plot(history.history['val_accuracy'], color='blue', label='Validation')
plt.legend()
plt.show()

"""### After adding BatchNormalization and Dropout layer:
Now it seems there is less gap between the train accuracy and validation accuracy. That means the model is performing a bit good after adding few normalization techniques
"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], color='red', label='train')
plt.plot(history.history['val_accuracy'], color='blue', label='Validation')
plt.legend()
plt.show()

"""We can even increase the validation accuracy by adding more normalization techniques like L2 regularization etc

## Lets test the model
"""

import cv2

test_img = cv2.imread('/content/dog.jpg')

plt.imshow(test_img)

test_img.shape

test_img = cv2.resize(test_img, (256,256))

test_input = test_img.reshape((1,256,256,3))

model.predict(test_input)

test_img = cv2.imread('/content/cat.jpg')

plt.imshow(test_img)

test_img.shape

test_img = cv2.resize(test_img, (256,256))

test_input = test_img.reshape((1,256,256,3))

model.predict(test_input)

